{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqj6FoToA/itaonPx5T1Fj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rituchoudhary67/12345/blob/main/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MgHemqSy5M6q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ddc65ea-a531-4f24-a7fb-958dd8da741a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in dataset: ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Rule1', 'Rule2', 'TP', 'Rule3', 'Classifier']\n",
            "Accuracy by KNN  : 0.7250293772032902\n",
            "Accuracy by DT   : 0.6850763807285546\n",
            "Accuracy by LogReg: 0.782608695652174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-2edbfcb1af9e>:46: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  df.fillna(method=\"ffill\", inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load the dataset from csv\n",
        "data = pd.read_csv(\"NSEI data set.csv\")\n",
        "\n",
        "# Debug: Print available columns\n",
        "print(\"Columns in dataset:\", data.columns.tolist())\n",
        "\n",
        "# Extract 'Date' and 'Volume' if they exist\n",
        "if 'Date' in data.columns:\n",
        "    D = data['Date']\n",
        "else:\n",
        "    print(\"Column 'Date' not found in dataset.\")\n",
        "    D = None\n",
        "\n",
        "if 'Volume' in data.columns:\n",
        "    V = data['Volume']\n",
        "else:\n",
        "    print(\"Column 'Volume' not found in dataset.\")\n",
        "    V = None\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Step 1: Preprocess the data\n",
        "# Drop unwanted columns if they exist\n",
        "drop_columns = ['Date', 'Volume', 'TP', 'Rule1', 'Rule2', 'Rule3']\n",
        "existing_drop_columns = [col for col in drop_columns if col in df.columns]\n",
        "df = df.drop(columns=existing_drop_columns)\n",
        "\n",
        "# Convert numeric columns to float after removing commas\n",
        "numeric_columns = ['Open', 'High', 'Low', 'Close']\n",
        "for col in numeric_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].replace({\",\": \"\"}, regex=True).astype(float)\n",
        "    else:\n",
        "        print(f\"Column '{col}' not found in dataset.\")\n",
        "\n",
        "# Handle missing values by forward filling\n",
        "df.fillna(method=\"ffill\", inplace=True)\n",
        "\n",
        "# Ensure the target variable 'Classifier' exists and is of type int\n",
        "if 'Classifier' in df.columns:\n",
        "    df['Classifier'] = df['Classifier'].astype(int)\n",
        "else:\n",
        "    raise KeyError(\"Column 'Classifier' not found in dataset.\")\n",
        "\n",
        "# Step 2: Define features and target variable\n",
        "X = df.drop(columns=['Classifier'])  # Features\n",
        "y = df['Classifier']                 # Target variable\n",
        "\n",
        "# Split data into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Step 3: Initialize classifiers\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "decision_tree = DecisionTreeClassifier(random_state=42)\n",
        "logreg = LogisticRegression(solver='liblinear', random_state=42)\n",
        "\n",
        "# Step 4: Train the models\n",
        "knn.fit(X_train, y_train)\n",
        "decision_tree.fit(X_train, y_train)\n",
        "logreg.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred_knn = knn.predict(X_test)\n",
        "y_pred_dt = decision_tree.predict(X_test)\n",
        "y_pred_logreg = logreg.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy scores\n",
        "print(\"Accuracy by KNN  :\", accuracy_score(y_test, y_pred_knn))\n",
        "print(\"Accuracy by DT   :\", accuracy_score(y_test, y_pred_dt))\n",
        "print(\"Accuracy by LogReg:\", accuracy_score(y_test, y_pred_logreg))\n",
        "\n",
        "# Step 6: Insert predictions into the test dataset\n",
        "df_test = X_test.copy()\n",
        "df_test['KNN_Pred'] = y_pred_knn\n",
        "df_test['DT_Pred'] = y_pred_dt\n",
        "df_test['LogReg_Pred'] = y_pred_logreg\n",
        "\n",
        "# If the original data contained a 'Date' column, merge it back (using indices)\n",
        "if D is not None:\n",
        "    df_test = df_test.merge(data[['Date']], left_index=True, right_index=True)\n",
        "\n",
        "# If you also wish to include the original 'Volume' column, uncomment the following:\n",
        "# if V is not None:\n",
        "#     df_test = df_test.merge(data[['Volume']], left_index=True, right_index=True)\n",
        "\n",
        "# Save the updated DataFrame with predictions to a CSV file\n",
        "df_test.to_csv(\"NSEI_3C_Predictions.csv\", index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_csv(\"NSEI data set.csv\")\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display missing values before handling\n",
        "print(\"Missing values in the dataset:\\n\", df.isnull().sum())\n",
        "\n",
        "# Drop unnecessary columns based on training data\n",
        "drop_columns = ['Date', 'Volume', 'TP', 'Rule1', 'Rule2', 'Rule3']\n",
        "df.drop(columns=[col for col in drop_columns if col in df.columns], inplace=True)\n",
        "\n",
        "# Convert numeric columns after removing commas\n",
        "numeric_columns = ['Open', 'High', 'Low', 'Close', 'Adj Close']\n",
        "for col in numeric_columns:\n",
        "    if col in df.columns:\n",
        "        df[col] = df[col].replace({\",\": \"\"}, regex=True).astype(float)\n",
        "\n",
        "# Fill missing values using forward fill\n",
        "df.ffill(inplace=True)\n",
        "\n",
        "# Display missing values after handling\n",
        "print(\"Missing values after preprocessing:\\n\", df.isnull().sum())\n",
        "\n",
        "# Ensure target column 'Classifier' exists\n",
        "if 'Classifier' not in df.columns:\n",
        "    raise KeyError(\"Column 'Classifier' not found in dataset.\")\n",
        "\n",
        "# Define features (X) and target variable (y)\n",
        "X = df.drop(columns=['Classifier'])\n",
        "y = df['Classifier'].astype(int)  # Ensure correct datatype\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=3),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(solver='liblinear', random_state=42)\n",
        "}\n",
        "\n",
        "# Function to train and test models\n",
        "def train_and_test(X, y, test_size):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=42, stratify=y\n",
        "    )\n",
        "\n",
        "    results = {}\n",
        "    for model_name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Compute evaluation metrics\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "        recall = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "        f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "        confusion = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "        # Store results\n",
        "        results[model_name] = {\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1 Score': f1,\n",
        "            'Confusion Matrix': confusion\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{model_name} Model Results (Test Split {int(test_size * 100)}%):\")\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1 Score: {f1:.4f}\")\n",
        "        print(f\"Confusion Matrix:\\n{confusion}\")\n",
        "        print(f\"Classification Report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Run tests for multiple split ratios\n",
        "split_ratios = [0.40, 0.30, 0.20]\n",
        "for test_size in split_ratios:\n",
        "    train_and_test(X, y, test_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhaAJP3_NUCV",
        "outputId": "5d043c20-ae4d-446c-a23c-3d871c7d24c4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values in the dataset:\n",
            " Date          0\n",
            "Open          0\n",
            "High          0\n",
            "Low           0\n",
            "Close         0\n",
            "Volume        0\n",
            "Rule1         0\n",
            "Rule2         0\n",
            "TP            0\n",
            "Rule3         0\n",
            "Classifier    0\n",
            "dtype: int64\n",
            "Missing values after preprocessing:\n",
            " Open          0\n",
            "High          0\n",
            "Low           0\n",
            "Close         0\n",
            "Classifier    0\n",
            "dtype: int64\n",
            "\n",
            "KNN Model Results (Test Split 40%):\n",
            "Accuracy: 0.6868\n",
            "Precision: 0.6840\n",
            "Recall: 0.6821\n",
            "F1 Score: 0.6827\n",
            "Confusion Matrix:\n",
            "[[487 286]\n",
            " [247 682]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.63      0.65       773\n",
            "           1       0.70      0.73      0.72       929\n",
            "\n",
            "    accuracy                           0.69      1702\n",
            "   macro avg       0.68      0.68      0.68      1702\n",
            "weighted avg       0.69      0.69      0.69      1702\n",
            "\n",
            "\n",
            "Decision Tree Model Results (Test Split 40%):\n",
            "Accuracy: 0.6563\n",
            "Precision: 0.6537\n",
            "Recall: 0.6541\n",
            "F1 Score: 0.6538\n",
            "Confusion Matrix:\n",
            "[[487 286]\n",
            " [299 630]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      0.63      0.62       773\n",
            "           1       0.69      0.68      0.68       929\n",
            "\n",
            "    accuracy                           0.66      1702\n",
            "   macro avg       0.65      0.65      0.65      1702\n",
            "weighted avg       0.66      0.66      0.66      1702\n",
            "\n",
            "\n",
            "Logistic Regression Model Results (Test Split 40%):\n",
            "Accuracy: 0.7808\n",
            "Precision: 0.7801\n",
            "Recall: 0.7765\n",
            "F1 Score: 0.7777\n",
            "Confusion Matrix:\n",
            "[[564 209]\n",
            " [164 765]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.73      0.75       773\n",
            "           1       0.79      0.82      0.80       929\n",
            "\n",
            "    accuracy                           0.78      1702\n",
            "   macro avg       0.78      0.78      0.78      1702\n",
            "weighted avg       0.78      0.78      0.78      1702\n",
            "\n",
            "\n",
            "KNN Model Results (Test Split 30%):\n",
            "Accuracy: 0.7006\n",
            "Precision: 0.6981\n",
            "Recall: 0.6956\n",
            "F1 Score: 0.6963\n",
            "Confusion Matrix:\n",
            "[[371 208]\n",
            " [174 523]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.64      0.66       579\n",
            "           1       0.72      0.75      0.73       697\n",
            "\n",
            "    accuracy                           0.70      1276\n",
            "   macro avg       0.70      0.70      0.70      1276\n",
            "weighted avg       0.70      0.70      0.70      1276\n",
            "\n",
            "\n",
            "Decision Tree Model Results (Test Split 30%):\n",
            "Accuracy: 0.6763\n",
            "Precision: 0.6749\n",
            "Recall: 0.6762\n",
            "F1 Score: 0.6750\n",
            "Confusion Matrix:\n",
            "[[391 188]\n",
            " [225 472]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.63      0.68      0.65       579\n",
            "           1       0.72      0.68      0.70       697\n",
            "\n",
            "    accuracy                           0.68      1276\n",
            "   macro avg       0.67      0.68      0.68      1276\n",
            "weighted avg       0.68      0.68      0.68      1276\n",
            "\n",
            "\n",
            "Logistic Regression Model Results (Test Split 30%):\n",
            "Accuracy: 0.7829\n",
            "Precision: 0.7819\n",
            "Recall: 0.7789\n",
            "F1 Score: 0.7800\n",
            "Confusion Matrix:\n",
            "[[426 153]\n",
            " [124 573]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.77      0.74      0.75       579\n",
            "           1       0.79      0.82      0.81       697\n",
            "\n",
            "    accuracy                           0.78      1276\n",
            "   macro avg       0.78      0.78      0.78      1276\n",
            "weighted avg       0.78      0.78      0.78      1276\n",
            "\n",
            "\n",
            "KNN Model Results (Test Split 20%):\n",
            "Accuracy: 0.7250\n",
            "Precision: 0.7226\n",
            "Recall: 0.7220\n",
            "F1 Score: 0.7223\n",
            "Confusion Matrix:\n",
            "[[266 120]\n",
            " [114 351]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.69      0.69       386\n",
            "           1       0.75      0.75      0.75       465\n",
            "\n",
            "    accuracy                           0.73       851\n",
            "   macro avg       0.72      0.72      0.72       851\n",
            "weighted avg       0.72      0.73      0.72       851\n",
            "\n",
            "\n",
            "Decision Tree Model Results (Test Split 20%):\n",
            "Accuracy: 0.6851\n",
            "Precision: 0.6822\n",
            "Recall: 0.6815\n",
            "F1 Score: 0.6818\n",
            "Confusion Matrix:\n",
            "[[248 138]\n",
            " [130 335]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.64      0.65       386\n",
            "           1       0.71      0.72      0.71       465\n",
            "\n",
            "    accuracy                           0.69       851\n",
            "   macro avg       0.68      0.68      0.68       851\n",
            "weighted avg       0.68      0.69      0.68       851\n",
            "\n",
            "\n",
            "Logistic Regression Model Results (Test Split 20%):\n",
            "Accuracy: 0.7826\n",
            "Precision: 0.7817\n",
            "Recall: 0.7784\n",
            "F1 Score: 0.7796\n",
            "Confusion Matrix:\n",
            "[[283 103]\n",
            " [ 82 383]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.73      0.75       386\n",
            "           1       0.79      0.82      0.81       465\n",
            "\n",
            "    accuracy                           0.78       851\n",
            "   macro avg       0.78      0.78      0.78       851\n",
            "weighted avg       0.78      0.78      0.78       851\n",
            "\n"
          ]
        }
      ]
    }
  ]
}